{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d480b778-9fe5-4ec1-9f8a-5fcb256560ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional GAN (CGAN) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200489d8-da84-4455-beb9-39deb4ae62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision datasets pillow scikit-learn matplotlib pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb472242-f9ca-4203-86f5-f1874f51a91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 14:02:41.239359: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-25 14:02:43.533015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748174564.367854    5928 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748174564.625252    5928 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748174566.144220    5928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748174566.144272    5928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748174566.144274    5928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748174566.144276    5928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-25 14:02:46.494869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | D Loss: 0.1962 | G Loss: 1.1470\n",
      "Epoch [2/10] | D Loss: 0.1198 | G Loss: 1.6021\n",
      "Epoch [3/10] | D Loss: 0.3381 | G Loss: 1.4070\n",
      "Epoch [4/10] | D Loss: 0.1582 | G Loss: 1.3190\n",
      "Epoch [5/10] | D Loss: 0.3479 | G Loss: 0.8758\n",
      "Epoch [6/10] | D Loss: 0.2178 | G Loss: 1.2153\n",
      "Epoch [7/10] | D Loss: 0.1390 | G Loss: 1.6391\n",
      "Epoch [8/10] | D Loss: 0.0823 | G Loss: 2.2249\n",
      "Epoch [9/10] | D Loss: 0.0465 | G Loss: 2.9046\n",
      "Epoch [10/10] | D Loss: 0.0402 | G Loss: 3.0568\n",
      "Generating 41 images for class '1'...\n",
      "Generating 41 images for class '2'...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "#  Load dataset\n",
    "dataset = load_dataset(\"yuighj123/covid-19-classification\")\n",
    "num_classes = len(set(dataset[\"train\"][\"label\"]))\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "#  Preprocess Dataset for CGAN\n",
    "class CustomCovidDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, split, transform=None):\n",
    "        self.items = hf_dataset[split]\n",
    "        self.transform = transform\n",
    "        self.images = self.items['image']\n",
    "        self.labels = self.items['label']\n",
    "        self.label2idx = {l: i for i, l in enumerate(set(self.labels))}\n",
    "        self.idx2label = {i: l for l, i in self.label2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(np.array(img))\n",
    "        img = img.convert('RGB')\n",
    "        label = self.label2idx[self.labels[idx]]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = CustomCovidDataset(dataset, split=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "#  Define CGAN\n",
    "latent_dim = 100\n",
    "image_shape = (3, 64, 64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, int(np.prod(image_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([noise, c], 1)\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), *image_shape)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_classes + int(np.prod(image_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([img.view(img.size(0), -1), c], 1)\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "#  Train CGAN\n",
    "n_epochs = 10\n",
    "os.makedirs(\"generated_samples\", exist_ok=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.size(0)\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        valid = torch.ones(batch_size, 1, device=device)\n",
    "        fake = torch.zeros(batch_size, 1, device=device)\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_D.zero_grad()\n",
    "        real_pred = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(real_pred, valid)\n",
    "        fake_pred = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        for class_idx in range(num_classes):\n",
    "            z = torch.randn(10, latent_dim, device=device)\n",
    "            labels = torch.full((10,), class_idx, dtype=torch.long, device=device)\n",
    "            with torch.no_grad():\n",
    "                fake_imgs = generator(z, labels)\n",
    "            utils.save_image(\n",
    "                fake_imgs,\n",
    "                f\"generated_samples/epoch_{epoch+1}_class_{class_idx}.png\",\n",
    "                nrow=5, normalize=True\n",
    "            )\n",
    "\n",
    "# Generate Synthetic Images for Balancing\n",
    "counts = dict(Counter(dataset[\"train\"][\"label\"]))\n",
    "max_count = max(counts.values())\n",
    "to_generate = {label: max_count - count for label, count in counts.items()}\n",
    "label2idx = train_ds.label2idx\n",
    "idx2label = train_ds.idx2label\n",
    "\n",
    "balanced_dataset = list(dataset[\"train\"])\n",
    "for label, n_gen in to_generate.items():\n",
    "    if n_gen == 0:\n",
    "        continue\n",
    "    print(f\"Generating {n_gen} images for class '{idx2label[label]}'...\")\n",
    "    z = torch.randn(n_gen, latent_dim, device=device)\n",
    "    gen_labels = torch.full((n_gen,), label, dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = generator(z, gen_labels)\n",
    "    # Convert synthetic images to PIL for CNN\n",
    "    fake_imgs = fake_imgs.cpu().numpy()  # Shape: (n_gen, 3, 64, 64)\n",
    "    fake_imgs = (fake_imgs + 1) / 2  # Denormalize to [0, 1]\n",
    "    fake_imgs = np.transpose(fake_imgs, (0, 2, 3, 1))  # Shape: (n_gen, 64, 64, 3)\n",
    "    for i in range(n_gen):\n",
    "        img_array = (fake_imgs[i] * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array)\n",
    "        balanced_dataset.append({\"image\": img, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676ab904-9fa7-4f6d-835c-115581cc46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (333, 224, 224, 3)\n",
      "Validation images shape: (66, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-25 14:20:48.996031: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/user/miniconda/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.4053 - loss: 2.4042 - val_accuracy: 0.3030 - val_loss: 1.0842\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - accuracy: 0.5073 - loss: 1.0124 - val_accuracy: 0.6970 - val_loss: 0.7598\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3s/step - accuracy: 0.6267 - loss: 0.8092 - val_accuracy: 0.6212 - val_loss: 0.7901\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.7527 - loss: 0.6529 - val_accuracy: 0.6818 - val_loss: 0.5748\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7326 - loss: 0.5363 - val_accuracy: 0.7273 - val_loss: 0.5197\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7429 - loss: 0.4973 - val_accuracy: 0.6970 - val_loss: 0.8129\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3s/step - accuracy: 0.7461 - loss: 0.5719 - val_accuracy: 0.8030 - val_loss: 0.5332\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.7816 - loss: 0.4449 - val_accuracy: 0.8030 - val_loss: 0.4861\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - accuracy: 0.7979 - loss: 0.4861 - val_accuracy: 0.8182 - val_loss: 0.4465\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.7932 - loss: 0.3840 - val_accuracy: 0.8636 - val_loss: 0.3402\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375ms/step - accuracy: 0.8498 - loss: 0.3783\n",
      "Validation Loss: 0.3402\n",
      "Validation Accuracy: 0.8636\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Covid       1.00      0.92      0.96        26\n",
      "         Normal       0.69      1.00      0.82        20\n",
      "Viral Pneumonia       1.00      0.65      0.79        20\n",
      "\n",
      "       accuracy                           0.86        66\n",
      "      macro avg       0.90      0.86      0.85        66\n",
      "   weighted avg       0.91      0.86      0.86        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Preprocess for CNN\n",
    "def preprocess_image_for_cnn(image, target_size=(224, 224)):\n",
    "    try:\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image.astype(np.uint8))\n",
    "        image = image.convert('RGB').resize(target_size)\n",
    "        image = np.array(image, dtype=np.float32) / 255.0\n",
    "        if image.shape != (224, 224, 3):\n",
    "            raise ValueError(f\"Image has unexpected shape: {image.shape}\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for sample in balanced_dataset:\n",
    "    img = preprocess_image_for_cnn(sample[\"image\"], target_size=(224, 224))\n",
    "    if img is not None:\n",
    "        train_images.append(img)\n",
    "        train_labels.append(sample[\"label\"])\n",
    "    else:\n",
    "        print(f\"Skipping invalid image for label {sample['label']}\")\n",
    "\n",
    "train_images = np.array(train_images, dtype=np.float32)\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "\n",
    "val_images = []\n",
    "val_labels = []\n",
    "for sample in dataset[\"test\"]:\n",
    "    img = preprocess_image_for_cnn(sample[\"image\"], target_size=(224, 224))\n",
    "    if img is not None:\n",
    "        val_images.append(img)\n",
    "        val_labels.append(sample[\"label\"])\n",
    "    else:\n",
    "        print(f\"Skipping invalid validation image for label {sample['label']}\")\n",
    "\n",
    "val_images = np.array(val_images, dtype=np.float32)\n",
    "val_labels = np.array(val_labels, dtype=np.int32)\n",
    "print(f\"Validation images shape: {val_images.shape}\")\n",
    "\n",
    "#  Train CNN\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(train_images, train_labels, batch_size=32)\n",
    "val_generator = val_datagen.flow(val_images, val_labels, batch_size=32)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "#  Evaluate\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "val_predictions = model.predict(val_images)\n",
    "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_pred_labels, target_names=label_names))\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
